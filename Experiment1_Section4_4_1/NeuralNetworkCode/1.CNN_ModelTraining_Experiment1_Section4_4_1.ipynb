{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Train the model for Experiment 1, 4.4.1 Multiclass Classification of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "#from models import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLA(\n",
      "  (base): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer3): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (level_1): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_node): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (right_node): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (prev_root): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer5): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (level_1): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_node): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (right_node): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (prev_root): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer6): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (linear2): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[-0.0469, -0.2142,  0.0197,  0.1608,  0.3539, -0.0318, -0.1227, -0.2860,\n",
      "          0.1394, -0.2714]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "'''DLA in PyTorch.\n",
    "Reference:\n",
    "    Deep Layer Aggregation. https://arxiv.org/abs/1707.06484\n",
    "'''\n",
    "'''\n",
    "Code taken from (with a permission published on the website https://github.com/kuangliu/pytorch-cifar/blob/master/LICENSE)\n",
    "https://github.com/kuangliu/pytorch-cifar/blob/master/models/dla.py\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tree(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.level = level\n",
    "        if level == 1:\n",
    "            self.root = Root(2*out_channels, out_channels)\n",
    "            self.left_node = block(in_channels, out_channels, stride=stride)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "        else:\n",
    "            self.root = Root((level+2)*out_channels, out_channels)\n",
    "            for i in reversed(range(1, level)):\n",
    "                subtree = Tree(block, in_channels, out_channels,\n",
    "                               level=i, stride=stride)\n",
    "                self.__setattr__('level_%d' % i, subtree)\n",
    "            self.prev_root = block(in_channels, out_channels, stride=stride)\n",
    "            self.left_node = block(out_channels, out_channels, stride=1)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [self.prev_root(x)] if self.level > 1 else []\n",
    "        for i in reversed(range(1, self.level)):\n",
    "            level_i = self.__getattr__('level_%d' % i)\n",
    "            x = level_i(x)\n",
    "            xs.append(x)\n",
    "        x = self.left_node(x)\n",
    "        xs.append(x)\n",
    "        x = self.right_node(x)\n",
    "        xs.append(x)\n",
    "        out = self.root(xs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DLA(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10):\n",
    "        super(DLA, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
    "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
    "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
    "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
    "        self.linear1 = nn.Linear(512, embed_dim)\n",
    "        self.linear2 = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1) \n",
    "        out = self.linear1(out)\n",
    "        emb = out\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = DLA()\n",
    "    print(net)\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "    y = net(x)\n",
    "    print(y)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to define the model architecture\n",
    "net = DLA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DLA(\n",
       "  (base): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Tree(\n",
       "    (root): Root(\n",
       "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (left_node): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (right_node): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Tree(\n",
       "    (root): Root(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (level_1): Tree(\n",
       "      (root): Root(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (left_node): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (right_node): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (prev_root): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (left_node): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (right_node): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer5): Tree(\n",
       "    (root): Root(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (level_1): Tree(\n",
       "      (root): Root(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (left_node): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (right_node): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (prev_root): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (left_node): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (right_node): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer6): Tree(\n",
       "    (root): Root(\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (left_node): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (right_node): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear1): Linear(in_features=512, out_features=16, bias=True)\n",
       "  (linear2): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the summary\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16294634"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of parameters\n",
    "\n",
    "[param.nelement() for param in net.parameters()]\n",
    "\n",
    "sum([param.nelement() for param in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation necessary to apply to the images\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in (or download) the test data and create a training sample \n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='/workspace/cifar-10-batches-py', train=True, download=False, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2) #batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the training data\n",
    "\n",
    "len(trainloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in (or download) the test data and create a test sample \n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='/workspace/SPC_Embeddings/cifar-10-batches-py', train=False, download=False, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2) #batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classes\n",
    "\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether to train on cpu or gpu, get the path, define the name of the network (here: net)\n",
    "\n",
    "net = DLA()\n",
    "device = 'cpu'\n",
    "data_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function and the optimization rules\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train process\n",
    "\n",
    "def train(epoch):\n",
    "    #print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*inputs.size(0) # to obtain loss for the whole batch, not an averaged value\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # For F1-Score\n",
    "        ypred_batch = outputs.max(dim=1)[1].detach().numpy().tolist()\n",
    "        ypred.extend(ypred_batch)\n",
    "        \n",
    "        ytrue_batch = targets.detach().numpy()\n",
    "        ytrue_batch = ytrue_batch.tolist()\n",
    "        ytrue.extend(ytrue_batch)\n",
    "        \n",
    "    return train_loss / len(trainloader.dataset), 100.*correct/total, sklearn.metrics.f1_score(ytrue, ypred, average='macro') # Values for the whole epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test process\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    ypred = []\n",
    "    ytrue = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()*inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # For F1-Score\n",
    "            ypred_batch = outputs.max(dim=1)[1].detach().numpy().tolist()\n",
    "            ypred.extend(ypred_batch)\n",
    "        \n",
    "            ytrue_batch = targets.detach().numpy()\n",
    "            ytrue_batch = ytrue_batch.tolist()\n",
    "            ytrue.extend(ytrue_batch)\n",
    "            \n",
    "    return test_loss / len(testloader.dataset), 100.*correct/total, sklearn.metrics.f1_score(ytrue, ypred, average='macro') # Values for the whole epoch       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial values and the patience criterium: Here, if for 10 epochs the accuracy on the test data does not increase,\n",
    "# stop the training of the network  \n",
    "\n",
    "f1score_max = 0.0\n",
    "epoch_patience = 10\n",
    "epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the way to name the produced interim models from the training\n",
    "\n",
    "def to_str(var):\n",
    "    return str(list(np.reshape(np.asarray(var), (1, np.size(var)))[0]))[1:-1]\n",
    "\n",
    "def truncate(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return int(n * multiplier) / multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.99, Test loss: 1.76, Train acc.: 23.0, Test acc.: 33.5, f1_train:0.204, f1_test:0.306\n",
      "Epoch: 2, Train loss: 1.62, Test loss: 1.52, Train acc.: 38.5, Test acc.: 42.7, f1_train:0.376, f1_test:0.409\n",
      "Epoch: 3, Train loss: 1.37, Test loss: 1.29, Train acc.: 49.5, Test acc.: 53.7, f1_train:0.485, f1_test:0.525\n",
      "Epoch: 4, Train loss: 1.16, Test loss: 1.21, Train acc.: 58.3, Test acc.: 57.2, f1_train:0.578, f1_test:0.562\n",
      "Epoch: 5, Train loss: 1.01, Test loss: 1.02, Train acc.: 64.0, Test acc.: 64.2, f1_train:0.638, f1_test:0.633\n",
      "Epoch: 6, Train loss: 0.895, Test loss: 0.877, Train acc.: 68.4, Test acc.: 69.1, f1_train:0.682, f1_test:0.69\n",
      "Epoch: 7, Train loss: 0.803, Test loss: 0.805, Train acc.: 71.7, Test acc.: 71.5, f1_train:0.716, f1_test:0.718\n",
      "Epoch: 8, Train loss: 0.717, Test loss: 0.783, Train acc.: 75.0, Test acc.: 73.2, f1_train:0.749, f1_test:0.73\n",
      "Epoch: 9, Train loss: 0.649, Test loss: 0.743, Train acc.: 77.4, Test acc.: 74.3, f1_train:0.774, f1_test:0.742\n",
      "Epoch: 10, Train loss: 0.59, Test loss: 0.689, Train acc.: 79.7, Test acc.: 76.3, f1_train:0.796, f1_test:0.761\n",
      "Epoch: 11, Train loss: 0.542, Test loss: 0.653, Train acc.: 81.2, Test acc.: 77.9, f1_train:0.812, f1_test:0.777\n",
      "Epoch: 12, Train loss: 0.5, Test loss: 0.825, Train acc.: 82.7, Test acc.: 73.8, f1_train:0.826, f1_test:0.739\n",
      "Epoch: 13, Train loss: 0.466, Test loss: 0.667, Train acc.: 83.8, Test acc.: 78.3, f1_train:0.838, f1_test:0.787\n",
      "Epoch: 14, Train loss: 0.438, Test loss: 0.558, Train acc.: 85.1, Test acc.: 81.5, f1_train:0.85, f1_test:0.813\n",
      "Epoch: 15, Train loss: 0.412, Test loss: 0.576, Train acc.: 85.7, Test acc.: 81.2, f1_train:0.857, f1_test:0.808\n",
      "Epoch: 16, Train loss: 0.383, Test loss: 0.573, Train acc.: 86.9, Test acc.: 81.4, f1_train:0.869, f1_test:0.812\n",
      "Epoch: 17, Train loss: 0.364, Test loss: 0.509, Train acc.: 87.5, Test acc.: 83.3, f1_train:0.875, f1_test:0.836\n",
      "Epoch: 18, Train loss: 0.346, Test loss: 0.54, Train acc.: 88.1, Test acc.: 82.5, f1_train:0.881, f1_test:0.824\n",
      "Epoch: 19, Train loss: 0.318, Test loss: 0.483, Train acc.: 89.0, Test acc.: 84.6, f1_train:0.89, f1_test:0.847\n",
      "Epoch: 20, Train loss: 0.309, Test loss: 0.525, Train acc.: 89.4, Test acc.: 83.0, f1_train:0.894, f1_test:0.831\n",
      "Epoch: 21, Train loss: 0.282, Test loss: 0.498, Train acc.: 90.3, Test acc.: 83.9, f1_train:0.903, f1_test:0.842\n",
      "Epoch: 22, Train loss: 0.273, Test loss: 0.487, Train acc.: 90.6, Test acc.: 84.7, f1_train:0.906, f1_test:0.848\n",
      "Epoch: 23, Train loss: 0.258, Test loss: 0.428, Train acc.: 91.2, Test acc.: 86.3, f1_train:0.912, f1_test:0.863\n",
      "Epoch: 24, Train loss: 0.246, Test loss: 0.467, Train acc.: 91.4, Test acc.: 85.0, f1_train:0.914, f1_test:0.85\n",
      "Epoch: 25, Train loss: 0.229, Test loss: 0.49, Train acc.: 92.1, Test acc.: 84.7, f1_train:0.921, f1_test:0.846\n",
      "Epoch: 26, Train loss: 0.222, Test loss: 0.503, Train acc.: 92.4, Test acc.: 84.6, f1_train:0.924, f1_test:0.848\n",
      "Epoch: 27, Train loss: 0.217, Test loss: 0.439, Train acc.: 92.4, Test acc.: 86.3, f1_train:0.924, f1_test:0.865\n",
      "Epoch: 28, Train loss: 0.201, Test loss: 0.419, Train acc.: 92.9, Test acc.: 87.4, f1_train:0.929, f1_test:0.874\n",
      "Epoch: 29, Train loss: 0.184, Test loss: 0.434, Train acc.: 93.6, Test acc.: 86.9, f1_train:0.936, f1_test:0.869\n",
      "Epoch: 30, Train loss: 0.18, Test loss: 0.443, Train acc.: 93.7, Test acc.: 86.7, f1_train:0.937, f1_test:0.867\n",
      "Epoch: 31, Train loss: 0.17, Test loss: 0.482, Train acc.: 94.1, Test acc.: 85.8, f1_train:0.941, f1_test:0.858\n",
      "Epoch: 32, Train loss: 0.161, Test loss: 0.469, Train acc.: 94.5, Test acc.: 86.4, f1_train:0.945, f1_test:0.863\n",
      "Epoch: 33, Train loss: 0.156, Test loss: 0.466, Train acc.: 94.5, Test acc.: 86.6, f1_train:0.945, f1_test:0.867\n",
      "Epoch: 34, Train loss: 0.149, Test loss: 0.443, Train acc.: 94.9, Test acc.: 87.4, f1_train:0.949, f1_test:0.875\n",
      "Epoch: 35, Train loss: 0.142, Test loss: 0.499, Train acc.: 95.1, Test acc.: 86.0, f1_train:0.951, f1_test:0.86\n",
      "Epoch: 36, Train loss: 0.134, Test loss: 0.432, Train acc.: 95.3, Test acc.: 87.9, f1_train:0.953, f1_test:0.879\n",
      "Epoch: 37, Train loss: 0.128, Test loss: 0.48, Train acc.: 95.6, Test acc.: 87.0, f1_train:0.956, f1_test:0.868\n",
      "Epoch: 38, Train loss: 0.122, Test loss: 0.483, Train acc.: 95.7, Test acc.: 87.0, f1_train:0.957, f1_test:0.871\n",
      "Epoch: 39, Train loss: 0.119, Test loss: 0.46, Train acc.: 95.9, Test acc.: 87.3, f1_train:0.959, f1_test:0.871\n",
      "Epoch: 40, Train loss: 0.113, Test loss: 0.411, Train acc.: 96.1, Test acc.: 88.5, f1_train:0.961, f1_test:0.885\n",
      "Epoch: 41, Train loss: 0.107, Test loss: 0.466, Train acc.: 96.2, Test acc.: 87.6, f1_train:0.962, f1_test:0.875\n",
      "Epoch: 42, Train loss: 0.0999, Test loss: 0.44, Train acc.: 96.5, Test acc.: 88.3, f1_train:0.965, f1_test:0.883\n",
      "Epoch: 43, Train loss: 0.097, Test loss: 0.489, Train acc.: 96.6, Test acc.: 87.3, f1_train:0.966, f1_test:0.872\n",
      "Epoch: 44, Train loss: 0.0959, Test loss: 0.453, Train acc.: 96.6, Test acc.: 87.8, f1_train:0.966, f1_test:0.877\n",
      "Epoch: 45, Train loss: 0.0863, Test loss: 0.452, Train acc.: 97.0, Test acc.: 88.8, f1_train:0.97, f1_test:0.886\n",
      "Epoch: 46, Train loss: 0.0831, Test loss: 0.466, Train acc.: 97.2, Test acc.: 87.9, f1_train:0.972, f1_test:0.878\n",
      "Epoch: 47, Train loss: 0.0842, Test loss: 0.447, Train acc.: 97.0, Test acc.: 88.5, f1_train:0.97, f1_test:0.885\n",
      "Epoch: 48, Train loss: 0.0794, Test loss: 0.444, Train acc.: 97.2, Test acc.: 88.5, f1_train:0.972, f1_test:0.884\n",
      "Epoch: 49, Train loss: 0.0724, Test loss: 0.522, Train acc.: 97.5, Test acc.: 87.2, f1_train:0.975, f1_test:0.871\n",
      "Epoch: 50, Train loss: 0.0716, Test loss: 0.467, Train acc.: 97.6, Test acc.: 88.2, f1_train:0.976, f1_test:0.881\n",
      "Epoch: 51, Train loss: 0.069, Test loss: 0.468, Train acc.: 97.7, Test acc.: 88.3, f1_train:0.977, f1_test:0.882\n",
      "Epoch: 52, Train loss: 0.0709, Test loss: 0.467, Train acc.: 97.6, Test acc.: 88.5, f1_train:0.976, f1_test:0.884\n",
      "Epoch: 53, Train loss: 0.0615, Test loss: 0.508, Train acc.: 97.9, Test acc.: 87.7, f1_train:0.979, f1_test:0.877\n",
      "Epoch: 54, Train loss: 0.061, Test loss: 0.464, Train acc.: 97.9, Test acc.: 88.8, f1_train:0.979, f1_test:0.887\n",
      "Epoch: 55, Train loss: 0.0621, Test loss: 0.492, Train acc.: 97.8, Test acc.: 88.2, f1_train:0.978, f1_test:0.881\n",
      "Epoch: 56, Train loss: 0.0562, Test loss: 0.463, Train acc.: 98.1, Test acc.: 88.8, f1_train:0.981, f1_test:0.888\n",
      "Epoch: 57, Train loss: 0.0558, Test loss: 0.45, Train acc.: 98.0, Test acc.: 88.8, f1_train:0.98, f1_test:0.888\n",
      "Epoch: 58, Train loss: 0.0523, Test loss: 0.449, Train acc.: 98.2, Test acc.: 89.5, f1_train:0.982, f1_test:0.895\n",
      "Epoch: 59, Train loss: 0.0496, Test loss: 0.462, Train acc.: 98.3, Test acc.: 89.0, f1_train:0.983, f1_test:0.889\n",
      "Epoch: 60, Train loss: 0.0516, Test loss: 0.449, Train acc.: 98.3, Test acc.: 89.1, f1_train:0.983, f1_test:0.891\n",
      "Epoch: 61, Train loss: 0.0503, Test loss: 0.464, Train acc.: 98.3, Test acc.: 89.3, f1_train:0.983, f1_test:0.894\n",
      "Epoch: 62, Train loss: 0.0454, Test loss: 0.453, Train acc.: 98.4, Test acc.: 89.3, f1_train:0.984, f1_test:0.893\n",
      "Epoch: 63, Train loss: 0.0477, Test loss: 0.497, Train acc.: 98.3, Test acc.: 88.8, f1_train:0.983, f1_test:0.889\n",
      "Epoch: 64, Train loss: 0.0412, Test loss: 0.441, Train acc.: 98.6, Test acc.: 89.5, f1_train:0.986, f1_test:0.895\n",
      "Epoch: 65, Train loss: 0.041, Test loss: 0.461, Train acc.: 98.5, Test acc.: 89.5, f1_train:0.986, f1_test:0.894\n",
      "Epoch: 66, Train loss: 0.0421, Test loss: 0.483, Train acc.: 98.5, Test acc.: 89.1, f1_train:0.985, f1_test:0.89\n",
      "Epoch: 67, Train loss: 0.0419, Test loss: 0.456, Train acc.: 98.6, Test acc.: 89.4, f1_train:0.986, f1_test:0.894\n",
      "Epoch: 68, Train loss: 0.0417, Test loss: 0.487, Train acc.: 98.6, Test acc.: 88.8, f1_train:0.986, f1_test:0.889\n",
      "Epoch: 69, Train loss: 0.0396, Test loss: 0.467, Train acc.: 98.6, Test acc.: 89.3, f1_train:0.986, f1_test:0.893\n",
      "Epoch: 70, Train loss: 0.038, Test loss: 0.488, Train acc.: 98.7, Test acc.: 89.3, f1_train:0.987, f1_test:0.892\n",
      "Epoch: 71, Train loss: 0.0368, Test loss: 0.445, Train acc.: 98.8, Test acc.: 89.7, f1_train:0.988, f1_test:0.897\n",
      "Epoch: 72, Train loss: 0.0352, Test loss: 0.462, Train acc.: 98.8, Test acc.: 89.3, f1_train:0.988, f1_test:0.892\n",
      "Epoch: 73, Train loss: 0.0303, Test loss: 0.492, Train acc.: 99.0, Test acc.: 89.2, f1_train:0.99, f1_test:0.892\n",
      "Epoch: 74, Train loss: 0.0312, Test loss: 0.463, Train acc.: 99.0, Test acc.: 89.8, f1_train:0.99, f1_test:0.898\n",
      "Epoch: 75, Train loss: 0.0294, Test loss: 0.491, Train acc.: 99.0, Test acc.: 89.3, f1_train:0.99, f1_test:0.893\n",
      "Epoch: 76, Train loss: 0.0286, Test loss: 0.46, Train acc.: 99.0, Test acc.: 89.6, f1_train:0.99, f1_test:0.896\n",
      "Epoch: 77, Train loss: 0.0321, Test loss: 0.466, Train acc.: 98.9, Test acc.: 89.9, f1_train:0.989, f1_test:0.899\n",
      "Epoch: 78, Train loss: 0.0323, Test loss: 0.538, Train acc.: 98.9, Test acc.: 88.5, f1_train:0.989, f1_test:0.884\n",
      "Epoch: 79, Train loss: 0.0294, Test loss: 0.498, Train acc.: 99.0, Test acc.: 89.3, f1_train:0.99, f1_test:0.892\n",
      "Epoch: 80, Train loss: 0.0326, Test loss: 0.517, Train acc.: 98.9, Test acc.: 88.9, f1_train:0.989, f1_test:0.889\n",
      "Epoch: 81, Train loss: 0.0299, Test loss: 0.46, Train acc.: 98.9, Test acc.: 89.7, f1_train:0.989, f1_test:0.897\n",
      "Epoch: 82, Train loss: 0.0227, Test loss: 0.481, Train acc.: 99.2, Test acc.: 89.5, f1_train:0.992, f1_test:0.895\n",
      "Epoch: 83, Train loss: 0.0264, Test loss: 0.466, Train acc.: 99.1, Test acc.: 89.7, f1_train:0.991, f1_test:0.898\n",
      "Epoch: 84, Train loss: 0.0282, Test loss: 0.466, Train acc.: 99.1, Test acc.: 89.6, f1_train:0.991, f1_test:0.896\n",
      "Epoch: 85, Train loss: 0.0281, Test loss: 0.452, Train acc.: 99.0, Test acc.: 90.1, f1_train:0.99, f1_test:0.901\n",
      "Epoch: 86, Train loss: 0.0266, Test loss: 0.443, Train acc.: 99.1, Test acc.: 90.1, f1_train:0.991, f1_test:0.9\n",
      "Epoch: 87, Train loss: 0.0245, Test loss: 0.466, Train acc.: 99.2, Test acc.: 90.1, f1_train:0.992, f1_test:0.9\n",
      "Epoch: 88, Train loss: 0.019, Test loss: 0.445, Train acc.: 99.4, Test acc.: 90.4, f1_train:0.994, f1_test:0.904\n",
      "Epoch: 89, Train loss: 0.023, Test loss: 0.498, Train acc.: 99.2, Test acc.: 89.5, f1_train:0.992, f1_test:0.895\n",
      "Epoch: 90, Train loss: 0.0231, Test loss: 0.521, Train acc.: 99.2, Test acc.: 89.2, f1_train:0.992, f1_test:0.893\n",
      "Epoch: 91, Train loss: 0.023, Test loss: 0.492, Train acc.: 99.2, Test acc.: 89.7, f1_train:0.992, f1_test:0.896\n",
      "Epoch: 92, Train loss: 0.0232, Test loss: 0.491, Train acc.: 99.2, Test acc.: 89.8, f1_train:0.992, f1_test:0.897\n",
      "Epoch: 93, Train loss: 0.0217, Test loss: 0.464, Train acc.: 99.3, Test acc.: 90.2, f1_train:0.993, f1_test:0.901\n",
      "Epoch: 94, Train loss: 0.0213, Test loss: 0.48, Train acc.: 99.3, Test acc.: 89.6, f1_train:0.993, f1_test:0.896\n",
      "Epoch: 95, Train loss: 0.0195, Test loss: 0.498, Train acc.: 99.4, Test acc.: 89.7, f1_train:0.994, f1_test:0.896\n",
      "Epoch: 96, Train loss: 0.0235, Test loss: 0.458, Train acc.: 99.2, Test acc.: 90.3, f1_train:0.992, f1_test:0.903\n",
      "Epoch: 97, Train loss: 0.0178, Test loss: 0.496, Train acc.: 99.5, Test acc.: 89.9, f1_train:0.995, f1_test:0.899\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "\n",
    "hist = {\"loss\":[], \"acc\":[], \"testloss\":[], \"test_acc\":[], \"f1_train\":[], \"f1_test\":[]}\n",
    "for epoch in range(1, 1001):\n",
    "    train_loss, train_acc, f1score_train = train(epoch)\n",
    "    test_loss, test_acc, f1score_test = test(epoch)\n",
    "   \n",
    "    hist[\"loss\"].append(train_loss)\n",
    "    hist[\"testloss\"].append(test_loss)\n",
    "    hist[\"acc\"].append(train_acc)\n",
    "    hist[\"test_acc\"].append(test_acc)\n",
    "    hist[\"f1_train\"].append(f1score_train)\n",
    "    hist[\"f1_test\"].append(f1score_test)\n",
    "    \n",
    "    if f1score_test > f1score_max:\n",
    "        torch.save(net.state_dict(), data_path +  to_str(truncate(f1score_test, decimals=3)) + 'DLA_CIFAR10.pt')\n",
    "        f1score_max = f1score_test\n",
    "        \n",
    "    #if epoch > 100: \n",
    "    if f1score_test < f1score_max:\n",
    "        epochs += 1\n",
    "        if epochs == epoch_patience:\n",
    "            break\n",
    "    elif f1score_test >= f1score_max:  \n",
    "        epochs = 0\n",
    "            \n",
    "    \n",
    "    print(f'Epoch: {epoch}, Train loss: {train_loss:.3}, Test loss: {test_loss:.3}, Train acc.: {train_acc:.3}, Test acc.: {test_acc:.3}, f1_train:{f1score_train:.3}, f1_test:{f1score_test:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9043093347866646"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final accuracy on the test data; This model is the final version that is implemented for creating the embeddings\n",
    "f1score_max"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
